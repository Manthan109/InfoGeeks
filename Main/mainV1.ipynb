{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mainV1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "npBfOE4lgqRd"
      },
      "source": [
        "#set up for selenium\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt03sOLYh8v7"
      },
      "source": [
        "# setup with tweets\n",
        "!pip install tweepy\n",
        "!pip install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTns8bMDiDqZ"
      },
      "source": [
        "!pip install PyGithub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXqfHsDFsyNA"
      },
      "source": [
        "!pip install pymp-pypi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYKu4wtOrLh5"
      },
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kujjdO-9rUQX",
        "outputId": "b65a79af-535c-4d65-b9c3-9f78aab5443c"
      },
      "source": [
        "# redirecting to google.com \n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        " # If you are using it in your local system, repalce the first argument with the path of your chromium.exe file.\n",
        " # The driver should be into the directory where python is installed. Add the driver's path in  the system variables path also.\n",
        " \n",
        "\n",
        "# function for getting links from the specified category\n",
        "def link_from_category(category_link, category, n_pages):\n",
        "  class_from_categ = {\"News\":\"dbsr\", \"Videos\":\"yuRUbf\"} #class tag for categories\n",
        "  class_tag = \"\"\n",
        "  class_tag = class_from_categ[category]\n",
        "\n",
        "  results = [] # list for storing all the links\n",
        "\n",
        "\n",
        "  for page in range(1, n_pages):\n",
        "    url = category_link +  str((page - 1) * 10) \n",
        "    driver.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    search = soup.find_all('div', class_=class_tag )\n",
        "\n",
        "    for h in search:\n",
        "        results.append(h.a.get('href'))\n",
        "\n",
        "    \n",
        "  return results\n",
        "\n",
        "def links_for_search(query, newslinks_results, n_pages=10):\n",
        "  newslinks_results = list(newslinks_results)\n",
        "\n",
        "  driver.get(\"https://www.google.com\")\n",
        "\n",
        "  # accessing the search bar and searching the specified query\n",
        "  search_bar = driver.find_element_by_name(\"q\")\n",
        "  search_bar.clear()\n",
        "  search_bar.send_keys(query)\n",
        "  search_bar.send_keys(Keys.RETURN)\n",
        "\n",
        "  # fetching the news and videos links for the specified query\n",
        "  category_list = [\"News\"]\n",
        "  category_link = []\n",
        "  for i in category_list:\n",
        "    category_link.append(driver.find_element_by_link_text(i).get_attribute('href'))\n",
        "\n",
        "\n",
        "  # fetching all the links for news articles\n",
        "  newslinks_results.append(link_from_category(category_link[0], \"News\",n_pages))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: use options instead of chrome_options\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VVPKkAlimWI"
      },
      "source": [
        "from extract_Videos import extractYoutubeVideos\n",
        "from extractrepos import extract_repos\n",
        "from sentiment_analysis_twitter import get_tweets_main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PSlVf8qir-8"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "videos_links = []\n",
        "extractYoutubeVideos(\"deep learning\",videos_links)\n",
        "\n",
        "repo_links = []\n",
        "extract_repos(\"deep learning\", repo_links)\n",
        "\n",
        "tweets = []\n",
        "get_tweets_main = get_tweets_main(\"deep learning\", tweets)\n",
        "\n",
        "news_links = []\n",
        "links_for_search(\"deep learning\", news_links)\n",
        "\n",
        "print(\"Time for serial implementation:\",time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQL8ygXDo1X0"
      },
      "source": [
        "from multiprocessing import Process\n",
        "\n",
        "query = input(\"What do you want to search? \")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "results = [ [] for i in range(4) ]\n",
        "\n",
        "p1 = multiprocessing.Process(target=extractYoutubeVideos, args=(query, results[0]))\n",
        "p2 = multiprocessing.Process(target=extract_repos, args=(query, results[1]))\n",
        "p3 = multiprocessing.Process(target=get_tweets_main, args=(query, results[2]))\n",
        "p4 = multiprocessing.Process(target=links_for_search, args=(query, results[3]))\n",
        "\n",
        "p1.start()\n",
        "p2.start()\n",
        "p3.start()\n",
        "p4.start()\n",
        "\n",
        "p1.join()\n",
        "p2.join()\n",
        "p3.join()\n",
        "p4.join()\n",
        "\n",
        "print(\"Time for parallel implementation:\",time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}